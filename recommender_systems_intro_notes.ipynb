{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"recommender_systems_intro_notes.ipynb","provenance":[],"authorship_tag":"ABX9TyMoZmjzitYA99LEFVX43ys2"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"5dbV7n-BUOcF"},"source":["#Recommendation Systems\n","- Systems/techniques that recommend or suggest a particular product, service or entity.\n","- Classified into the following two categories, based on their approach to providing recommendations.\n","1. The Prediction Problem\n","- Given a matrix of m users and n items:\n","    - Each row of the matrix represents a user and the column represents an item(shape = mn * nn)\n","    - Value on the cell in the ith row and jth column denotes the rating given by user i to the item j; denoted as rij\n","    - The matrix can be dense or sparse depending on the number of ratings provided\n","- The prediction problem aims to predict the missing values using all the information it has at its disposal(i.e ratings recorded, data on users, data on items, etc).\n","- If it predicts the values accurately, it will be able to give good recommendations\n","\n","2. The Ranking Problem\n","- More intuitive formulation.\n","- Given a set of n items, the ranking problem tries to discern the top k items to recommend to a particular user m using all the information at its disposal.\n","- The prediction problem often boils down to the ranking problem i.e if we are able to predict the missing values, we can extract the top k values.\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"JqtIucdBVRVF"},"source":["# Types of Recommendation Systems\n","-In recommener systems, the techniques and models to use are largely dependent on the quantity and quality of data.\n","\n","# 1. Collaborative Filtering\n","- Leaverages the power of community to provide reccomendations.\n","- Classifies into types:\n","\n","# a) User-based Collaborative Filtering\n","- The model recommends items to a user that similar users liked e.g customers who bought this also bought this.\n","\n","# b) Item-based Collaborative Filtering\n","- Works on the principle that if a group of people have rated two items similarly, then the two items must be similar, hence if a person likes one item they might be interested in the other item too e.g \n","- The models recommend items based on previous browsing and purchase history and past ratings of users.\n","\n","# Shortcomings\n","One of the biggest prerequisites of a collaborative filtering system is the\n","availability of data of past activity. Therefore, collaborative filters suffer from the cold start problem i.e to build a good collaborative filtering system, you need data on a large number of purchases from a large number of users which is not available to you at the beginning and it's therefore difficult to build such a system from the start."]},{"cell_type":"markdown","metadata":{"id":"VwWtJqSdZs7l"},"source":["# 2. Content-based Systems\n","- Does not require data from past activity.\n","- Provide recommendations based on a user profile and metadata it has on particular items.\n","- e.g The first time you sign in to Netflix, it doesn't know what your likes and dislikes are, so it is not in a position to find users similar to you and recommend the movies and shows they have liked.what Netflix does instead is ask you to rate a few movies that you have watched before and based on this\n","information and the metadata it already has on movies, it creates a watchlist\n","for you.\n","\n","# Shortcomings\n","- However, since content-based systems don't leverage the power of the\n","community, they often come up with results that are not as impressive or\n","relevant as the ones offered by collaborative filters. In other words, contentbased systems usually provide recommendations that are obvious."]},{"cell_type":"markdown","metadata":{"id":"Gce_wMNJa0nq"},"source":["# 3. Knowledge-based Recommenders\n","- Used for items that are rarely bought making it impossible to recommend such items based on past purchasing activity or by building a user profile e.g real estate.\n","- In such cases, you build a system that asks for certain specifics and\n","preferences and then provides recommendations that satisfy those conditions.\n","\n","# Shortcomings\n","- Knowledge-based recommenders suffer from the problem of low\n","novelty. Users know full-well what to expect from the results and\n","are seldom taken by surprise."]},{"cell_type":"markdown","metadata":{"id":"IsRx9gQTcox9"},"source":["# 4. Hybrid Recommenders\n","- More robust recomenders that combine various types of recommendation systems\n","- The try to nullify the disadvantage of one model against an advantage of another. \n","- Consider Netflix, When you sign in for the first time, it overcomes the cold start problem of collaborative filters by using a content-based recommender, and, as you gradually start watching and rating movies, it brings its collaborative filtering mechanism into play.\n","- This is far more successful, so most practical recommender systems are\n","hybrid in nature."]},{"cell_type":"markdown","metadata":{"id":"puXzPSMs-ZxE"},"source":["# Document Vectors\n","- Essentially, the models we are building compute the pairwise similarity between bodies of text. But how do we numerically quantify the similarity\n","between two bodies of text?\n","- To put it another way, consider three movies: A, B, and C. How can we\n","mathematically prove that the plot of A is more similar to the plot of B than\n","to that of C (or vice versa)? \n","- But what are the values of these vectors? The answer to that question\n","depends on the vectorizer we are using to convert our documents into\n","vectors. The two most popular vectorizers are CountVectorizer and TFIDFVectorizer.\n","\n","1. The first step toward answering these questions is to represent the bodies of\n","text (henceforth referred to as documents) as mathematical quantities. \n","- This is done by representing these documents as vectors. In other words, every document is depicted as a series of n numbers, where each number represents a dimension and n is the size of the vocabulary of all the\n","documents put together.\n","- But what are the values of these vectors? The answer to that question\n","depends on the vectorizer we are using to convert our documents into\n","vectors. The two most popular vectorizers are CountVectorizer and TFIDFVectorizer.\n"]},{"cell_type":"markdown","metadata":{"id":"4Zy0_5lADW2u"},"source":["# 1. CountVectorizer \n","- CountVectorizer is the simplest type of vectorizer\n","- The first step is to compute the size of the vocabulary. The vocabulary is the number of unique words present across all documents.\n","- It is common practice to not include extremely common words such as a,\n","the, is, had, my, and so on (also known as stop words) in the vocabulary.\n","- The document will be represented as an n-dimensional vector, where each dimension represents the number of times a particular word occurs in a document.\n","\n","#2 TF-IDF Vectorizer(Term Frequency-Inverse Document Frequency)\n","- Assigns weights to each word according to the following formula. For every word i in document j, the following applies:\n","        Wij = tfij * log(N / dfi)\n","\n","    where:\n","    - W i,j is the weight of word i in document j\n","    - dfi is the number of documents that contain the term i\n","    - N is the total number of documents\n","\n","- The weight of a word in a document is greater if it occurs more frequently in that document and is present in fewer documents. \n","- The weight W i,j takes values between 0 and 1\n","- TF-IDFVectorizer is preferred because some words occur much more frequently in plot descriptions than others and is therefore a good idea to assign weights to each word in a document according to the TF-IDF formula.\n","- Also TF-IDF speeds up the calculation of the cosine similarity score between a pair of documents."]},{"cell_type":"markdown","metadata":{"id":"GTpqFpS8tthp"},"source":["# The Cosine Similarity Score\n","- Extremely robust and easy to calculate especiall when used with TF-IDF Vectorizer\n","- The cosine similarity score between 2 documents x and y is as follows:\n","        cosine(x, y) = (x.yT) / (||x|| . ||y||)\n","- The cosine score takes values between -1 and 1\n","- The higher the cosine, the more similar the documents are"]},{"cell_type":"markdown","metadata":{"id":"rJo60XW24G6c"},"source":["#COLLABORATIVE FILTERING"]},{"cell_type":"markdown","metadata":{"id":"cA5Oz5oXo8Yk"},"source":["# Data Mining Techniques\n","\n","1. Similariity Measures:\n","Given 2 items, how do we mathematically quantify how different or similar they are to one another.\n","\n","2. Dimensionality Reduction:\n","To improve perfomance, speed up calculations and avoid the curse of dimensionality, it is often a good idea to reduce the number of dimensions i.e features, considerably while still retaining most of the information.\n","\n","3. Supervised Learning:\n","This is a class of Machine Learning algorithms that make use of labeled historical data to infer a mapping function that can be used to predict the label/class of unlabeled data.\n","- They include:\n","    - Support Vector Machines\n","    - Decision Trees\n","    - Ensemble Models\n","    - Regression Models\n","\n","4. Clustering:\n","Type of unsupervised learning technique where the algorithm tries to divide all the data points into a certain number of clusters, therefore without the prensence of labeled data, the algorithm is able to assign clusters to all unlabeled points\n","- They include:\n","    - K-Mean Clustering\n","    - Fuzzy C-Means Clustering\n","\n","5. Evaluation Methods and Metrics:\n","These are used to gauge the perfomance of the algorithms\n","- They include: \n","    - Accuracy\n","    - Precision\n","    - Recall\n","    - F1\n","\n"]},{"cell_type":"markdown","metadata":{"id":"Bsbd0ttqvK0M"},"source":["# 1. Similarity Measures\n","\n","#a) Euclidean Distance\n","- Defined as the length of the line segment joinig 2 data points plotted on an n-dimensional Cartesian plane/Mathematical space\n","- The score can take any value between 0 and infinity\n","- the lower the score, the more similar the vectors are to each other"]},{"cell_type":"code","metadata":{"id":"E2r8Mjiqy9E4"},"source":["def euclidean(vector1, vector2):\n","    # convert 1-d python lists to numpy vectors\n","    vector1 = np.array(vector1)\n","    vector_2 = np.array(vector2)\n","    \n","    # compute vector which is the element-wise square of the difference\n","    diff = np.power(np.array(vector1) - np.array(vector2), 2)\n","    sigma_val = np.sum(diff)\n","    euclidean_score = np.sqrt(sigma_val)\n","    \n","    return euclidean_score"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4LAgJ7p_zHw_"},"source":["# b) Pearson Correlation\n","- euclidean Distances place emphasis on magnitude and are not able to gauge the degree of similarity or dissimilarity well.\n","- Te Pearson correlation is a score between -1 and 1 where -1 indicates total negative correlation, 1 indicated total positive correlation and 0 indicated no corrlation at all i.e the two entities anre independent of each other."]},{"cell_type":"code","metadata":{"id":"wfz-qqtl1ndY"},"source":["from scipy.stats import pearsonr\n","\n","pearson_score = pearsonr(vector1, vector2)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Kw9dDJx11xez"},"source":["# c) Cosine Similarity\n","- Computes the cosine of the angle between 2 vectors in an n-dimensional mathematical space.\n","- If the cosine is 1(angle is 0), the vectors are exactly similar\n","- if the cosine is -1(angle is 180), the vectors are exactly dissimilar to one another. \n","-Consider two vectors, x and y, both with zero mean.In this case, the Pearson correlation score is exactly the same as the cosine similarity Score. i.e for centered vectors with zero mean, the Pearson correlation is the cosine similarity score."]},{"cell_type":"markdown","metadata":{"id":"GugNp3zl3mVP"},"source":["# NOTE:\n","Different similarity scores are appropriate in different scenarios. \n","- For cases where the magnitude is important, the Euclidean distance is an appropriate metric to use. \n","- However, magnitude is not as important to us as correlation.\n","- Therefore, we will be using the Pearson and the cosine similarity scores\n","when building our filters."]},{"cell_type":"markdown","metadata":{"id":"ifp57xdx35Va"},"source":["# 2. Clustering\n","One of the main ideas behind collaborative filtering is that if user A has the sam opinion on an item as user B then A is also likely to have the same opinion as B on another item than that of a randomly chosen user.\n","- Clustering is a popular technique used in collaborative filtering algorithms.\n","- An unsupervised learning algorithm that groups data oints into different classes in such a way that data points belonging to a particular class are more similar to each other than those belonging to a different class\n","- The job of a clustering algorithm is toassign classes to every pont on the cartesian plane\n","- There is no one clustering algorithm to rule them all, each algorithm has its specific use case and is suitable only in certain problems.\n"]},{"cell_type":"markdown","metadata":{"id":"BZIqxheG6QXy"},"source":["# a) K-Means Clustering\n","- Takes the data points and number of clusters as input.\n","- Next it randomly plots k different points on the cartesian plane known as centroids.\n","- After the k centroids are randomly plotted, the following 2 steps are perfomed iteratively until it as achieved convergence i.e no further changes in the set of k centroids:\n","    1. Assignment of points to the centroids: \n","    - Every data point is assigned to the centroid that is the closest to it.\n","The collection of data points assigned to a particular centroid is called a cluster. Therefore, the assignment of points to k centroids results in the formation of k clusters.\n","    2. Reassignment of centroids: \n","    - In the next step, the centroid of every cluster is recomputed to be the center of the cluster (or the average of all the points in the cluster). All the data points are then reassigned to the new centroids\n","\n"]},{"cell_type":"markdown","metadata":{"id":"PCY5MSpfGAfu"},"source":["# 3. Dimensionality Reduction\n","\n","- Most machine learning algorithms tend to perform poorly as the number of dimensions in the data increases. This phenomenon is often known as\n","the curse of dimensionality. Therefore, it is a good idea to reduce the\n","number of features available in the data, while retaining the maximum\n","amount of information possible. \n","- There are two ways to achieve this:\n","    1. Feature selection: \n","    - This method involves identifying the features that have the least predictive power and dropping them altogether. Therefore, feature selection involves identifying a subset of features that is most important for that particular use case. \n","    - An important distinction of feature selection is that it maintains the original meaning of every retained feature. \n","    2. Feature extraction: \n","    - Feature extraction takes in m-dimensional data and transforms it into an n-dimensional output space (usually where m >> n), while retaining most of the information. However, in doing so, it creates new features that have no inherent meaning.\n","- An example is the Principle Component Analysis(PCA)"]},{"cell_type":"markdown","metadata":{"id":"kL_UJD6hHnpK"},"source":["# a) Principal Componenet Analysis (PCA)\n","- Unsupervised feature_extraction algorithm that takes in m-dimensional input and creates a set of n(m >> n) linearly uncorrelated variables called Principal Components, in such a way that the n-dimensions lose as little variance/information as possible due to the loss of the m-n dimensions\n","- The linear transformation in PCA is done in such a way that the first\n","principal component holds the maximum variance (or information). It does\n","so by considering those variables that are highly correlated to each other.\n","- Every principal component has more variance than every succeeding\n","component and is orthogonal to the preceding component."]},{"cell_type":"markdown","metadata":{"id":"csxO9zsZNXHM"},"source":["# b) Linear-Discriminant Analysis\n","Like PCA, LDA is a linear transformation method that aims to transform m-dimensional data into an n-dimensional output space.\n","- However, unlike PCA, which tries to retain the maximum information,\n","LDA aims to identify a set of n features that result in the maximum\n","separation (or discrimination) of classes.\n","- Since LDA requires labeled data in order to determine its components, it is a type of supervised learning algorithm."]},{"cell_type":"markdown","metadata":{"id":"v1i8cFOoQOdw"},"source":["# c) Singular Value Decomposition (SVD)\n","- Is a type of matrix analysis technique that allows us to represent a high-dimensional matrix in a lower dimension. SVD achieves this by identifying and removing the less important parts of the matrix and producing an approximation in the desired number of dimensions."]},{"cell_type":"markdown","metadata":{"id":"rvZYgpU1RANX"},"source":["# 4. Supervised Learning\n","- Supervised learning is a class of machine learning algorithm that takes in a\n","series of vectors and their corresponding output (a continuous value or a\n","class) as input, and produces an inferred function that can be used to map\n","new examples.\n","- An important precondition for using supervised learning is the availability\n","of labeled data i.e it is necessary that we have access to input\n","for which we already know the correct output.\n","- Supervised learning can be classified into two types: \n","    1. Classification:\n","    - A classification problem has a discrete set of values as the target\n","variable (for instance, a like and a dislike),\n","    2. Regression: \n","    - Regression problem has a continuous value as its target (for instance, an average rating between one and five).\n","- Consider a matrix m. It is possible to treat (m-1) columns as the input and the mth column as the target variable. In this way, it should be possible to predict an unavailable value in the mth column by passing in the corresponding (m-1) dimensional vector."]},{"cell_type":"markdown","metadata":{"id":"NGhOnAPxSZXM"},"source":["# a) K-Nearest Neighbors (k-NN)\n","- In the case of classification, it assigns a class to a particular data\n","point by a majority vote of its k nearest neighbors i.e the data\n","point is assigned the class that is the most common among its k-nearest\n","neighbors. \n","- In the case of regression, it computes the average value for the\n","target variable based on its k-nearest neighbors.\n","- Unlike most machine learning algorithms, k-NN is non-parametric and lazy in nature:\n","    - The former means that k-NN does not make any underlying assumptions about the distribution of the data i.e the model structure is determined by the data\n","    - The latter means that k-NN undergoes virtually no training. It only computes the k-nearest neighbors of a particular point in the prediction phase. This also means that the k-NN model needs to have access to the training data at all times and cannot discard it during prediction like its sister algorithms\n","\n","- in k-NN classification, Consider a dataset that has binary classes. k-NN now plots this into n-dimensional space (in this case, two dimensions). \n","- Before the k-NN algorithm can make predictions, it needs to know the number of nearest neighbors that have to be taken into consideration (the value of k)\n","    - k is usually odd (to avoid ties in the case of binary classification).\n","- Consider the case where k=3. k-NN computes the distance metric (usually the Euclidean distance) from the new point to every other point in the training dataset and selects the three data points that are closest to it.\n","- The next step is to determine the majority class among the three points and assigns the new point to it.\n","\n","- k-NN regression works in almost the same way. Instead of classes, we compute the property values of the k-NN.\n","- Imagine that we have a housing dataset and we're trying to predict the price\n","of a house. The price of a particular house will therefore be determined by\n","the average of the prices of the houses of its k nearest neighbors.\n","- As with classification, the final target value may differ depending on the value of k\n","\n","# NOTE:\n","The value of k is extremely significant in determining the final class assigned to a data point. It is often a good practice to test different values of k and assess its performance with your cross-validation and test datasets."]},{"cell_type":"markdown","metadata":{"id":"wthaLhjNVVVB"},"source":["# b) Support Vector Machines\n","\n","- It takes in an n-dimensional dataset as input and constructs an (n-1) dimensional hyperplane in such a way that there is maximum separation of classes.\n","- The SVM model is only dependent on support vectors; these are the points\n","that determine the maximum margin possible between the two classes. The rest of the points do not have an effect on the workings of the SVM\n","- SVMs are also capable of separating classes that are not linearly separable. It does so with special tools, called radial kernel functions, that plot the points in a higher dimension and attempt to construct a maximum margin hyperplane there."]},{"cell_type":"markdown","metadata":{"id":"oaczSGs3XeAe"},"source":["# c) Decision Trees\n","- Decision trees are extremely fast and simple tree-based algorithms that\n","branch out on features that result in the largest information gain. \n","- Decision trees, although not very accurate, are extremely interpretable.\n","- Decision trees have an element of randomness in their workings and come\n","up with different conditions in different iterations. "]},{"cell_type":"markdown","metadata":{"id":"-HxeyacFYCQ8"},"source":["# d) Ensembling\n","- The main idea behind ensembling is that the predictive power of multiple algorithms is much greater than a singe algorithm\n","\n","# Bagging \n","- Bagging is short for bootstrap aggregation:\n","- Like most ensemble methods, it leverages over a large number of base classification models and averages their results to deliver its final prediction.\n","- These are the steps involved in building a bagging model:\n","1. A certain percentage of the data points are sampled (say 10%). The\n","Sampling is done with replacement. In other words, a particular data\n","point can appear in multiple iterations.\n","2. A baseline classification model (typically a decision tree) is trained on\n","this sampled data.\n","3. This process is repeated until n number of models are trained. The\n","final prediction delivered by the bagging model is the average of all\n","the predictions of all the base models.\n","\n","# Random Forests\n","- An improvement on the bagging model is the random forest model. \n","- In addition to sampling data points, the random forest ensemble method also\n","forces each baseline model to randomly select a subset of the features (usually a number equal to the square root of the total number of features)\n","- Selecting a subset of samples, as well as features, to build the baseline\n","decision trees greatly enhances the randomness of each individual tree.\n","- This, in turn, increases the robustness of the random forest and allows it to\n","perform extremely well with noisy data.\n","- Additionally, building baseline models from a subset of features and\n","analyzing their contribution to the final prediction also allows the random\n","forest to determine the importance of each feature. It is therefore possible to\n","perform feature-selection using random forests (recall that feature-selection\n","is a type of dimensionality reduction).\n","\n","# Boosting \n","- The bagging and the random forest models train baseline models that are\n","completely independent of each other. Therefore, they do not learn from the\n","mistakes that each learner has made. This is where boosting comes into\n","play.\n","- Like random forests, boosting models build a baseline model using a subset\n","of samples and features. However, while building the next learners, the\n","boosting model tries to rectify the mistakes that the previous learners made.\n","- Different boosting algorithms do this in different ways.\n","- Boosting algorithms are extremely robust and routinely provide high\n","performance.\n"]},{"cell_type":"markdown","metadata":{"id":"btF-RGsTb0eR"},"source":["# 5. Evaluation Metrics\n","# a) Accuracy:\n","- Accuracy is the most widely used metric to gauge the performance of a\n","classification model. \n","- It is the ratio of the number of correct predictions to the total number of predictions made by the model\n","        accuracy = (true positives + true negatives) / total no. of predictions\n","\n","#b) Root Mean Square Error(RMSE):\n","- Metric widely used to gauge the performance of regressors.\n","\n","\n","- Sometimes, accuracy does not give us a good estimate of the performance\n","of a model, for such cases, we make use of other metrics. To understand them, we first need to define a few terms:\n","1. True positive (TP): True positive refers to all cases where the actual\n","and the predicted classes are both positive\n","2. True negative (TN): True negative refers to all cases where the actual\n","and the predicted classes are both negative\n","3. False positive (FP): These are all the cases where the actual class is\n","negative but the predicted class is positive\n","4. False negative (FN): These are all the cases where the actual class is\n","positive but the predicted class is negative\n","\n","# c) Precision:\n","- The precision is the ratio of the number of positive cases that were correct\n","to all the cases that were identified as positive. Mathematically, it looks like\n","this:\n","        precision = true positives / (false positives + true positives)\n","\n","# d) Recall:\n","- The recall is the ratio of positive cases that were identified to all positive cases pesent in the dataset\n","        recall = true positives / (true positives + false negatives)\n","\n","# e) F1 Score:\n","- This score conveys the balance between precision and recall i.e is the harmonic mean of the precision and recall.\n","- An F1 score of 1 implies perfect precicion and recall \n","- An F1 score of 0 implies precision and recall are not possible\n","        F1 =2. ( (precision * recall) / (precision + recall))"]}]}